{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h3 align=\"center\">__Module 9 Activity__</h3>\n",
    "# <h3 align=\"center\">__Assigned at the start of Module 9__</h3>\n",
    "# <h3 align=\"center\">__Due at the end of Module 9__</h3><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly Discussion Forum Participation\n",
    "\n",
    "Each week, you are required to participate in the module’s discussion forum. The discussion forum consists of the week's Module Activity, which is released at the beginning of the module. You must complete/attempt the activity before you can post about the activity and anything that relates to the topic. \n",
    "\n",
    "## Grading of the Discussion\n",
    "\n",
    "### 1. Initial Post:\n",
    "Create your thread by **Day 5 (Saturday night at midnight, PST).**\n",
    "\n",
    "### 2. Responses:\n",
    "Respond to at least two other posts by **Day 7 (Monday night at midnight, PST).**\n",
    "\n",
    "---\n",
    "\n",
    "## Grading Criteria:\n",
    "\n",
    "Your participation will be graded as follows:\n",
    "\n",
    "### Full Credit (100 points):\n",
    "- Submit your initial post by **Day 5.**\n",
    "- Respond to at least two other posts by **Day 7.**\n",
    "\n",
    "### Half Credit (50 points):\n",
    "- If your initial post is late but you respond to two other posts.\n",
    "- If your initial post is on time but you fail to respond to at least two other posts.\n",
    "\n",
    "### No Credit (0 points):\n",
    "- If both your initial post and responses are late.\n",
    "- If you fail to submit an initial post and do not respond to any others.\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Notes:\n",
    "\n",
    "- **Late Initial Posts:** Late posts will automatically receive half credit if two responses are completed on time.\n",
    "- **Substance Matters:** Responses must be thoughtful and constructive. Comments like “Great post!” or “I agree!” without further explanation will not earn credit.\n",
    "- **Balance Participation:** Aim to engage with threads that have fewer or no responses to ensure a balanced discussion.\n",
    "\n",
    "---\n",
    "\n",
    "## Avoid:\n",
    "- A number of posts within a very short time-frame, especially immediately prior to the posting deadline.\n",
    "- Posts that complement another post, and then consist of a summary of that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Investigating FEature Importance in Decision Tree & Random Forest Regression\n",
    "\n",
    "You are working with a housing dataset where your goal is to predict house prices based on different features like square footage, number of bedrooms, and location. You will train a Decision Tree Regressor and a Random Forest Regressor and compare how each model determines feature importance.\n",
    "\n",
    "## Code\n",
    "\n",
    "```python\n",
    "# Generate a synthetic dataset with more features for a Car Price Prediction Model\n",
    "np.random.seed(2)\n",
    "n_samples = 200\n",
    "\n",
    "engine_size = np.random.rand(n_samples) * 5  # Engine size in liters\n",
    "horsepower = np.random.rand(n_samples) * 125  # Horsepower\n",
    "weight = np.random.rand(n_samples) * 4000  # Car weight in pounds\n",
    "age = np.random.randint(1, 20, n_samples)  # Age of car in years\n",
    "mileage = np.random.rand(n_samples) * 200000  # Miles driven\n",
    "luxury_rating = np.random.randint(1, 10, n_samples)  # Subjective luxury rating\n",
    "\n",
    "# Target variable: Price of the car in USD\n",
    "y = (\n",
    "    2500 * engine_size +\n",
    "    250 * horsepower -\n",
    "    3 * weight -\n",
    "    800 * age -\n",
    "    0.05 * mileage +\n",
    "    1000 * luxury_rating +\n",
    "    np.random.normal(0, 5000, n_samples)  # Adding noise\n",
    ")\n",
    "\n",
    "# Create a dataframe\n",
    "df_cars = pd.DataFrame({\n",
    "    \"Engine_Size\": engine_size,\n",
    "    \"Horsepower\": horsepower,\n",
    "    \"Weight\": weight,\n",
    "    \"Age\": age,\n",
    "    \"Mileage\": mileage,\n",
    "    \"Luxury_Rating\": luxury_rating,\n",
    "    \"Price\": y\n",
    "})\n",
    "\n",
    "# Split dataset into features and target variable\n",
    "X = df_cars.drop(columns=[\"Price\"])\n",
    "y = df_cars[\"Price\"]\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Train Decision Tree and Random Forest models\n",
    "dt_model = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "dt_model.fit(X_scaled, y)\n",
    "rf_model.fit(X_scaled, y)\n",
    "\n",
    "# Extract feature importance\n",
    "dt_importance = dt_model.feature_importances_\n",
    "rf_importance = rf_model.feature_importances_\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Decision Tree Importance\": dt_importance,\n",
    "    \"Random Forest Importance\": rf_importance\n",
    "})\n",
    "\n",
    "# Sort by importance values (Random Forest)\n",
    "importance_df = importance_df.sort_values(by=\"Random Forest Importance\", ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=\"Random Forest Importance\", y=\"Feature\", data=importance_df, palette=\"mako\", hue=\"Feature\", legend=False)\n",
    "plt.xlabel(\"Feature Importance Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance in Random Forest - Car Price Prediction\")\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=\"Decision Tree Importance\", y=\"Feature\", data=importance_df, palette=\"rocket\", hue=\"Feature\", legend=False)\n",
    "plt.xlabel(\"Feature Importance Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance in Decision Tree - Car Price Prediction\")\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "\n",
    "## Questions\n",
    "1. Compare the feature importance scores from the Decision Tree and Random Forest models. Are they similar or different? Why might this be the case?\n",
    "2. Which feature has the highest importance in predicting house prices? Does this make sense logically? You have the \"pricing function\" try messing with the values. \n",
    "3. Try removing the least important feature from the dataset and retrain the model. Does the model’s performance change significantly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Investigating Multicollinearity and Regularization in Regression Models\n",
    "\n",
    "Multicollinearity can make linear regression models unstable. In this activity, you'll calculate Variance Inflation Factor (VIF) to identify multicollinearity and apply Ridge & Lasso Regression to handle it.\n",
    "\n",
    "## Code\n",
    "\n",
    "```python\n",
    "\n",
    "# Create multicollinear features\n",
    "X6 = engine_size + np.random.normal(0, 0.1, n_samples)  # Correlated with engine_size\n",
    "X7 = horsepower + np.random.normal(0, 5, n_samples)  # Correlated with horsepower\n",
    "\n",
    "# Update dataframe with new correlated features\n",
    "df_cars[\"Engine_Size_Correlated\"] = X6\n",
    "df_cars[\"Horsepower_Correlated\"] = X7\n",
    "\n",
    "# Update feature set\n",
    "X_multi = df_cars.drop(columns=[\"Price\"])\n",
    "y_multi = df_cars[\"Price\"]\n",
    "\n",
    "# Standardize the new dataset\n",
    "scaler = StandardScaler()\n",
    "X_multi_scaled = pd.DataFrame(scaler.fit_transform(X_multi), columns=X_multi.columns)\n",
    "\n",
    "# Compute VIF to check for multicollinearity\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X_multi_scaled.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_multi_scaled.values, i) for i in range(X_multi_scaled.shape[1])]\n",
    "\n",
    "# Train models\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_cars, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "ridge = Ridge(alpha=10).fit(X_train, y_train)\n",
    "lasso = Lasso(alpha=0.1).fit(X_train, y_train)\n",
    "\n",
    "# Print coefficients\n",
    "coef_df = pd.DataFrame({\"Feature\": df_cars.columns, \"LinearRegression\": lr.coef_, \n",
    "                         \"Ridge\": ridge.coef_, \"Lasso\": lasso.coef_})\n",
    "\n",
    "# Display tables\n",
    "print(\"VIF Scores:\")\n",
    "display(vif_data)\n",
    "print(\"\\nModel Coefficients:\")\n",
    "display(coef_df)\n",
    "\n",
    "```\n",
    "\n",
    "## Questions\n",
    "1. Look at the VIF scores for the features. Which feature has the highest multicollinearity?\n",
    "2. Compare the coefficients of Linear Regression vs. Ridge vs. Lasso. How do Ridge and Lasso modify the coefficients to handle multicollinearity?\n",
    "3. Try increasing the alpha value for Ridge and Lasso. How does this change the coefficients? What happens when you set alpha too high?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Comparing Decision Tree Regression and Support Vector Regression for Nonlinear data\n",
    "\n",
    "Support vector regression can model data that is lienar and non-linear by using different kernel functions. The linear kernel works best when there is a linear relationship between features and the target variable. Radial Basis Function kernels can understand non-linear patterns similar to decision trees. You can also smooth the step functions of a decision tree by using random forest regression. In this activity, you'll compare their performance on a nonlinear dataset.\n",
    "\n",
    "## Code\n",
    "\n",
    "```python\n",
    "# Generate Nonlinear Data\n",
    "np.random.seed(42)\n",
    "X_nl = np.sort(5 * np.random.rand(n_samples, 1), axis=0)\n",
    "y_nl = np.sin(X_nl).ravel() + np.random.normal(0, 0.1, X_nl.shape[0])\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_nl, y_nl, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Support Vector Regression models\n",
    "svr_linear = SVR(kernel='linear', C=1.0)\n",
    "svr_linear.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Train Decision Tree Regression model and Random Forest\n",
    "tree_reg = DecisionTreeRegressor(max_depth=5)\n",
    "forest_reg = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "tree_reg.fit(X_train, y_train)\n",
    "forest_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_svr_linear = svr_linear.predict(X_test)\n",
    "\n",
    "y_pred_tree = tree_reg.predict(X_test)\n",
    "y_pred_forest = forest_reg.predict(X_test)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "## Tasks\n",
    "1. Compute the mean squared error of the models.\n",
    "2. Plot the decision tree vs the random forest model\n",
    "3. Plot the linear kernel SVR against the decision tree or random forest.\n",
    "4. Add the radial basis function kernel SVR to the code. \n",
    "5. Compute the mean squared error of all the models again. \n",
    "6. Plot the SVR with rbf kernel against all other models. \n",
    "7. Answer questions below.\n",
    "\n",
    "\n",
    "## Questions\n",
    "1. Compare the MSE (Mean Squared Error) of all models. Which model performs better?\n",
    "2. Look at the plot of the fitted curves. Compare and contrast all the models smoothness curves.\n",
    "3. Try increasing the depth of the Decision Tree (max_depth=10) or the number of iterations of the random forest. What happens to the predictions? Do you observe overfitting?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
