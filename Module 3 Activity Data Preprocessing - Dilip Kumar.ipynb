{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# <h3 align=\"center\">__Module 3 Activity__</h3>\n# <h3 align=\"center\">__Assigned at the start of Module 3__</h3>\n# <h3 align=\"center\">__Due at the end of Module 3__</h3><br>\n\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Weekly Discussion Forum Participation\n\nEach week, you are required to participate in the module’s discussion forum. The discussion forum consists of the week's Module Activity, which is released at the beginning of the module. You must complete/attempt the activity before you can post about the activity and anything that relates to the topic. \n\n## Grading of the Discussion\n\n### 1. Initial Post:\nCreate your thread by **Day 5 (Saturday night at midnight, PST).**\n\n### 2. Responses:\nRespond to at least two other posts by **Day 7 (Monday night at midnight, PST).**\n\n---\n\n## Grading Criteria:\n\nYour participation will be graded as follows:\n\n### Full Credit (100 points):\n- Submit your initial post by **Day 5.**\n- Respond to at least two other posts by **Day 7.**\n\n### Half Credit (50 points):\n- If your initial post is late but you respond to two other posts.\n- If your initial post is on time but you fail to respond to at least two other posts.\n\n### No Credit (0 points):\n- If both your initial post and responses are late.\n- If you fail to submit an initial post and do not respond to any others.\n\n---\n\n## Additional Notes:\n\n- **Late Initial Posts:** Late posts will automatically receive half credit if two responses are completed on time.\n- **Substance Matters:** Responses must be thoughtful and constructive. Comments like “Great post!” or “I agree!” without further explanation will not earn credit.\n- **Balance Participation:** Aim to engage with threads that have fewer or no responses to ensure a balanced discussion.\n\n---\n\n## Avoid:\n- A number of posts within a very short time-frame, especially immediately prior to the posting deadline.\n- Posts that complement another post, and then consist of a summary of that.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Module Activity: Building a Preprocessing Pipeline\n\n## Objective\nLearn how to build a preprocessing pipeline in scikit-learn and apply it to the famous Iris dataset. Gain hands-on experience in handling missing values, scaling features, and understanding the importance of preprocessing pipelines.\n\n---\n\n## Sample Code for Pipeline Syntax\nHere’s an example to help you understand how to create a pipeline. This pipeline imputes missing values using the mean:\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\n# Example dataset with missing values\ndata = pd.DataFrame({\n    'Feature1': [1.0, np.nan, 3.0],\n    'Feature2': [np.nan, 2.0, 3.0]\n})\n\n# Define a pipeline with an imputer\npipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='mean'))\n])\n\n# Fit and transform the data\nprocessed_data = pipeline.fit_transform(data)\n\nprint(\"Original Data:\")\nprint(data)\nprint(\"\\nProcessed Data:\")\nprint(processed_data)\n\n\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Activity Instructions\n\n## Dataset Preparation\nWe will use the Iris dataset, randomly remove values to simulate missing data, and keep it in a Pandas DataFrame for you to preprocess.\n\n---\n\n## Your Task\nBuild a preprocessing pipeline that:\n- Imputes missing values using the median.\n- Scales features to a `[0, 1]` range using `MinMaxScaler`.\n- Add at least one more preprocessing step.\n\n### Reflection\nAt the end of the activity, answer the following questions:\n1. What challenges did you face while handling missing data?\n2. Why is it important to use a pipeline for preprocessing?\n---\n\n## Dataset Setup\nRun the following code to import the Iris dataset and simulate missing data. You will use this dataset for the activity.\n\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_iris\n\n# Load the Iris dataset\niris = load_iris()\ndata = pd.DataFrame(iris.data, columns=iris.feature_names)\n\n# Randomly introduce missing values in random cells\nnp.random.seed(42)\ntotal_cells = data.size\nnum_missing = int(0.1 * total_cells)  # 10% of total cells\nmissing_indices = [(row, col) for row in range(data.shape[0]) for col in range(data.shape[1])]\nrandom_missing_indices = np.random.choice(len(missing_indices), size=num_missing, replace=False)\n\nfor index in random_missing_indices:\n    row, col = missing_indices[index]\n    data.iat[row, col] = np.nan\n\nprint(\"Dataset with Missing Values:\")\nprint(data.head(10))\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Dataset with Missing Values:\n   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n0                5.1               3.5                NaN               0.2\n1                4.9               3.0                1.4               0.2\n2                4.7               3.2                NaN               0.2\n3                4.6               3.1                1.5               0.2\n4                5.0               3.6                1.4               0.2\n5                5.4               3.9                1.7               0.4\n6                NaN               3.4                1.4               0.3\n7                5.0               NaN                NaN               0.2\n8                4.4               2.9                1.4               0.2\n9                4.9               3.1                1.5               0.1\n"
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": "## Next Steps\n\n1. **Build your pipeline** to preprocess the dataset.\n2. **Test your pipeline** by fitting it to the Iris dataset and transforming it.\n3. **Review the processed data** and reflect on how the pipeline simplifies your workflow.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.preprocessing import MinMaxScaler # To Scale features\nfrom sklearn.preprocessing import QuantileTransformer # To capture non-linear relationships for models that are inherently linear\nfrom sklearn.impute import SimpleImputer # To handle potential missing values\nfrom sklearn.pipeline import Pipeline\n\n# 1. Define the preprocess steps\npipeline_steps = [\n    ('imputer', SimpleImputer(strategy='median')), # Step 1: Impute NaNs with median\n    ('quantile_transform', QuantileTransformer(output_distribution='normal')), # Step 2: Transforms features using quantile information. It can spread out the most frequent values and reduce the impact of (marginal) outliers. Can map to a uniform or normal distribution.\n    ('min_max_scaler', MinMaxScaler()), # Step 3: Scale features to [0, 1]\n]\n\n# 2. Create the pipeline\nnumerical_pipeline = Pipeline(steps=pipeline_steps)\n\n# Fit and transform the data\ndata_processed_np = numerical_pipeline.fit_transform(data)\n\ndata_processed_df = pd.DataFrame(data_processed_np, columns=data.columns, index=data.index)\ndata_processed_df.head(10)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:2785: UserWarning: n_quantiles (1000) is greater than the total number of samples (150). n_quantiles is set to n_samples.\n  warnings.warn(\n"
        },
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n0           0.426220          0.612608           0.500000          0.384116\n1           0.385771          0.494334           0.377125          0.384116\n2           0.348052          0.552671           0.500000          0.384116\n3           0.331942          0.534713           0.407396          0.384116\n4           0.407396          0.626628           0.377125          0.384116\n5           0.449199          0.680585           0.434628          0.439637\n6           0.500809          0.590064           0.377125          0.427301\n7           0.407396          0.494334           0.500000          0.384116\n8           0.287112          0.451968           0.377125          0.384116\n9           0.385771          0.534713           0.407396          0.000000",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.426220</td>\n      <td>0.612608</td>\n      <td>0.500000</td>\n      <td>0.384116</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.385771</td>\n      <td>0.494334</td>\n      <td>0.377125</td>\n      <td>0.384116</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.348052</td>\n      <td>0.552671</td>\n      <td>0.500000</td>\n      <td>0.384116</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.331942</td>\n      <td>0.534713</td>\n      <td>0.407396</td>\n      <td>0.384116</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.407396</td>\n      <td>0.626628</td>\n      <td>0.377125</td>\n      <td>0.384116</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.449199</td>\n      <td>0.680585</td>\n      <td>0.434628</td>\n      <td>0.439637</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.500809</td>\n      <td>0.590064</td>\n      <td>0.377125</td>\n      <td>0.427301</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.407396</td>\n      <td>0.494334</td>\n      <td>0.500000</td>\n      <td>0.384116</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.287112</td>\n      <td>0.451968</td>\n      <td>0.377125</td>\n      <td>0.384116</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.385771</td>\n      <td>0.534713</td>\n      <td>0.407396</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}