{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS0AqpD-wYYE"
      },
      "source": [
        "# <h3 align=\"center\">__Module 5 Activity__</h3>\n",
        "# <h3 align=\"center\">__Assigned at the start of Module 5__</h3>\n",
        "# <h3 align=\"center\">__Due at the end of Module 5__</h3><br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SwYbyf1wYYG"
      },
      "source": [
        "# Weekly Discussion Forum Participation\n",
        "\n",
        "Each week, you are required to participate in the module’s discussion forum. The discussion forum consists of the week's Module Activity, which is released at the beginning of the module. You must complete/attempt the activity before you can post about the activity and anything that relates to the topic.\n",
        "\n",
        "## Grading of the Discussion\n",
        "\n",
        "### 1. Initial Post:\n",
        "Create your thread by **Day 5 (Saturday night at midnight, PST).**\n",
        "\n",
        "### 2. Responses:\n",
        "Respond to at least two other posts by **Day 7 (Monday night at midnight, PST).**\n",
        "\n",
        "---\n",
        "\n",
        "## Grading Criteria:\n",
        "\n",
        "Your participation will be graded as follows:\n",
        "\n",
        "### Full Credit (100 points):\n",
        "- Submit your initial post by **Day 5.**\n",
        "- Respond to at least two other posts by **Day 7.**\n",
        "\n",
        "### Half Credit (50 points):\n",
        "- If your initial post is late but you respond to two other posts.\n",
        "- If your initial post is on time but you fail to respond to at least two other posts.\n",
        "\n",
        "### No Credit (0 points):\n",
        "- If both your initial post and responses are late.\n",
        "- If you fail to submit an initial post and do not respond to any others.\n",
        "\n",
        "---\n",
        "\n",
        "## Additional Notes:\n",
        "\n",
        "- **Late Initial Posts:** Late posts will automatically receive half credit if two responses are completed on time.\n",
        "- **Substance Matters:** Responses must be thoughtful and constructive. Comments like “Great post!” or “I agree!” without further explanation will not earn credit.\n",
        "- **Balance Participation:** Aim to engage with threads that have fewer or no responses to ensure a balanced discussion.\n",
        "\n",
        "---\n",
        "\n",
        "## Avoid:\n",
        "- A number of posts within a very short time-frame, especially immediately prior to the posting deadline.\n",
        "- Posts that complement another post, and then consist of a summary of that.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VO4H9V4wYYH"
      },
      "source": [
        "# Activity: Extending and Analyzing the Expectation-Maximization Algorithm\n",
        "\n",
        "This activity is designed to deepen your understanding of the Expectation-Maximization (EM) algorithm and its computational efficiency. Below, you will find a code implementation of the EM algorithm for a Gaussian Mixture Model. Your task is twofold:\n",
        "\n",
        "## Task 1: Extend the Algorithm\n",
        "Modify the code to run over multiple iterations. Implement:\n",
        "- **A maximum iteration count** to limit the number of iterations.\n",
        "- **A stopping criterion** based on convergence (e.g., a small change in parameter values between iterations).\n",
        "\n",
        "## Task 2: Analyze Runtime\n",
        "Add line-by-line runtime comments to the code to measure the execution time of each section. Using these measurements, derive the **runtime complexity** of the algorithm in terms of the number of data points (\\(n\\)) and components (\\(k\\)).\n",
        "\n",
        "---\n",
        "\n",
        "## Instructions\n",
        "\n",
        "### Part 1: Extending the Algorithm\n",
        "1. Add a `for` loop or `while` loop to iterate over the Expectation-Maximization steps until:\n",
        "   - A **maximum number of iterations** is reached (e.g., 100).\n",
        "   - The parameter values (`mu`, `sigma`, `pk`) converge, i.e., the change in values is below a predefined threshold (e.g., \\(10^{-4}\\)).\n",
        "\n",
        "2. Modify the algorithm to check for convergence at the end of each iteration:\n",
        "   - Use a metric like the **norm of the difference in means** (`mu`) or the **log-likelihood** of the data.\n",
        "   - Print the number of iterations the algorithm required before convergence.\n",
        "\n",
        "---\n",
        "\n",
        "### Part 2: Analyzing Runtime\n",
        "1. Import Python's `time` module and measure the runtime of each section of the code:\n",
        "   - **Initialization**: Measure the time to compute the initial `mu`, `sigma`, and `pk`.\n",
        "   - **E-Step**: Measure the time to compute probabilities and numerators.\n",
        "   - **M-Step**: Measure the time to update the parameters.\n",
        "\n",
        "2. Add comments next to each runtime measurement to document how long the operation took.\n",
        "\n",
        "3. Using the runtimes, analyze the algorithm's complexity:\n",
        "   - Consider \\(n\\), the number of data points.\n",
        "   - Consider \\(k\\), the number of Gaussian components.\n",
        "   - Derive the overall runtime complexity as a function of \\(n\\) and \\(k\\).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5VjcFjIwYYH",
        "outputId": "3744a95a-4a66-49f4-eb53-a3062663ccd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x [[1 2]\n",
            " [4 2]\n",
            " [1 3]\n",
            " [4 3]]\n",
            "column_means [2.5 2.5]\n",
            "column_stddevs [1.73205081 0.57735027]\n",
            "std_deviations [1.15470054 1.15470054]\n",
            "mu [[2.17662611 2.3922087 ]\n",
            " [3.75694927 2.91898309]]\n",
            "sigma [[1.15470054 1.15470054]]\n",
            "pk [[0.5 0.5]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([array([1.6232562 , 2.47791081]), array([3.69828951, 2.5301904 ])],\n",
              " [np.float64(0.9302774776426431), np.float64(0.7291058883137361)],\n",
              " [np.float64(0.5774796490102341), np.float64(0.4225203509897659)])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Define the initial data columns\n",
        "column_1 = np.array([1, 4, 1, 4])\n",
        "column_2 = np.array([2, 2, 3, 3])\n",
        "\n",
        "# Create a 2-column dataset\n",
        "x = np.column_stack((column_1, column_2))\n",
        "print(f\"x {x}\")\n",
        "\n",
        "# Compute the means and sample standard deviations of each column\n",
        "column_means = np.mean(x, axis=0)\n",
        "print(f\"column_means {column_means}\")\n",
        "\n",
        "column_stddevs = np.std(x, axis=0, ddof=1)  # Sample standard deviation (ddof=1)\n",
        "print(f\"column_stddevs {column_stddevs}\")\n",
        "\n",
        "# Compute the average of the standard deviations across columns\n",
        "average_of_values = np.mean(column_stddevs)\n",
        "\n",
        "# Create an array of uniform standard deviations using the average value\n",
        "std_deviations = np.full_like(column_stddevs, average_of_values)\n",
        "print(f\"std_deviations {std_deviations}\")\n",
        "\n",
        "# Initialize means (mu) based on the data\n",
        "# cluster_mean = overall_mean + (k * overall_std_dev)\n",
        "# k = np.array([-0.1867, 0.7257])\n",
        "mu = ((column_means.reshape(1, -1) * np.array([1, 1]).reshape(-1, 1)) +\n",
        "      (column_stddevs.reshape(1, -1) * np.array([-0.1867, 0.7257]).reshape(-1, 1)))\n",
        "print(f\"mu {mu}\")\n",
        "\n",
        "# Initialize standard deviations (sigma) for cluster\n",
        "sigma = std_deviations.reshape(1, -1) * np.array([1, 1]).reshape(1, -1)\n",
        "print(f\"sigma {sigma}\")\n",
        "\n",
        "# Initialize the prior probabilities (pk) i.e. Mixing Weights\n",
        "pk = np.array([1, 1]).reshape(1, -1) / 2\n",
        "print(f\"pk {pk}\")\n",
        "\n",
        "# Define the Gaussian probability density function\n",
        "def g(x, mu, sigma):\n",
        "    temp = 1 / ((((2 * math.pi) ** 0.5) * sigma) ** 2)  # Gaussian normalization constant\n",
        "    temp2 = (np.linalg.norm(x - mu) / sigma) ** 2       # Squared Mahalanobis distance\n",
        "    temp3 = np.exp(-0.5 * temp2)                        # Exponential factor\n",
        "    return temp * temp3\n",
        "\n",
        "# Helper function: Sum every k-th value in a list\n",
        "def sum_every_kth_value_list(arr, k):\n",
        "    result = []\n",
        "    for i in range(k):\n",
        "        sum_val = sum(arr[i::k])\n",
        "        result.append(sum_val)\n",
        "    return result\n",
        "\n",
        "# Helper function: Sum values in chunks of size k\n",
        "def sum_every_k_values(arr, k):\n",
        "    if k <= 0 or len(arr) % k != 0:\n",
        "        return \"Invalid input\"\n",
        "    return [sum(arr[i:i + k]) for i in range(0, len(arr), k)]\n",
        "\n",
        "# Helper function: Get the n-th set of k values from a list\n",
        "def get_nth_set_of_k_values(arr, k, n):\n",
        "    if k <= 0 or n <= 0:\n",
        "        return \"Invalid input\"\n",
        "    start_index = (n - 1) * k\n",
        "    end_index = start_index + k\n",
        "    return arr[start_index:end_index]\n",
        "\n",
        "# E-step: Calculate numerators for updating probabilities\n",
        "numerators = []\n",
        "for j in range(0, mu.shape[0]):  # Iterate over each Gaussian component\n",
        "    for i in range(0, x.shape[0]):  # Iterate over each data point\n",
        "        value = g(x[i], mu[j], sigma[0][j]) * pk[0][j]\n",
        "        numerators.append(value)\n",
        "\n",
        "# Compute denominators for normalizing probabilities\n",
        "denominators = sum_every_kth_value_list(numerators, int(len(numerators) / 2))\n",
        "\n",
        "# Update probabilities for each data point and each component\n",
        "new_p = []\n",
        "for i in range(0, len(numerators)):\n",
        "    new_p.append(numerators[i] / denominators[i % int(len(numerators) / 2)])\n",
        "\n",
        "# Compute p_k_n (updated prior probabilities)\n",
        "p_k_n = sum_every_k_values(new_p, int(len(numerators) / 2))\n",
        "\n",
        "# M-step: Update the means (mu) and standard deviations (sigma)\n",
        "new_mu = []\n",
        "new_std = []\n",
        "for i in range(1, 3):  # Iterate over components\n",
        "    temp = 0\n",
        "    temp2 = 0\n",
        "    for j in range(0, x.shape[0]):  # Update mean\n",
        "        temp += get_nth_set_of_k_values(new_p, 4, i)[j] * x[j]\n",
        "    temp = temp / p_k_n[i - 1]\n",
        "    new_mu.append(temp)\n",
        "\n",
        "    for k in range(0, x.shape[0]):  # Update standard deviation\n",
        "        temp2 += get_nth_set_of_k_values(new_p, 4, i)[k] * (np.linalg.norm(x[k] - new_mu[i - 1]) ** 2)\n",
        "    temp2 = (temp2 / (2 * p_k_n[i - 1])) ** 0.5\n",
        "    new_std.append(temp2)\n",
        "\n",
        "# Update prior probabilities\n",
        "updated_p = []\n",
        "for i in range(0, 2):\n",
        "    updated_p.append(p_k_n[i] / int(len(numerators) / 2))\n",
        "\n",
        "# Final updated parameters: new means, new standard deviations, and updated priors\n",
        "new_mu, new_std, updated_p\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understand the given code\n",
        "# 1.0 Analyze the data and initializing the paramters\n",
        "**Number of Clusters (or Mixtures):** We have 2 clusters. Let's call them Cluster A and Cluster B.\n",
        "\n",
        "**Number of Features (or Dimensions) in our Data:** Our data x has 2 columns (features). Each data point is a 2D coordinate, like [1, 2].\n",
        "```\n",
        "[[1 2]\n",
        " [4 2]\n",
        " [1 3]\n",
        " [4 3]]\n",
        "```\n",
        "\n",
        "## 1.1 Why mu is (2, 2)?\n",
        "```\n",
        "[[2.17662611 2.3922087 ]\n",
        " [3.75694927 2.91898309]]\n",
        "```\n",
        "The mean (mu) describes the center of a cluster. Since our data points live in a 2-dimensional space, the center of each cluster must also be a 2-dimensional coordinate.\n",
        "\n",
        "\n",
        "## 1.2 pk (Mixing Weights) - Shape (1, 2)\n",
        "```\n",
        "[[0.5 0.5]]\n",
        "```\n",
        "pk represents the overall probability or proportion of the entire dataset belonging to a cluster. Therefore it is a single number per cluster.\n",
        "\n",
        "- pk[0] = Probability of being in Cluster A (e.g., 0.5)\n",
        "- pk[1] = Probability of being in Cluster B (e.g., 0.5)\n",
        "\n",
        "## 1.3 sigma (Standard Deviations) - Shape (1, 2)\n",
        "```\n",
        "[[1.15470054 1.15470054]]\n",
        "```\n",
        "Here, we assumes a simple type of Gaussian distribution called a Spherical or Isotropic Gaussian.\n",
        "\n",
        "This means we assume that the spread (standard deviation) of a cluster is the same in all directions (for all features).\n",
        "\n",
        "So, for sigma, the code is designed to hold:\n",
        "- A single standard deviation value that applies to all features of Cluster A.\n",
        "- A single standard deviation value that applies to all features of Cluster B.\n",
        "\n",
        "## 1.4 What would sigma look like if we didn't make this assumption?\n",
        "If we allowed each feature in each cluster to have its own standard deviation (a \"Diagonal\" GMM), then sigma would have the exact same shape as mu: (2, 2).\n",
        "\n",
        "```\n",
        "# For a more complex \"Diagonal\" GMM, sigma would be (2, 2)\n",
        "sigma = [[std_dev_for_cluster_A_feature1, std_dev_for_cluster_A_feature2],\n",
        "         [std_dev_for_cluster_B_feature1, std_dev_for_cluster_B_feature2]]\n",
        "```\n",
        "\n",
        "# 2. Understand Helper functions\n",
        "## 2.1 Gaussian probability density function\n",
        "```\n",
        "def g(x, mu, sigma):\n",
        "    temp = 1 / ((((2 * math.pi) ** 0.5) * sigma) ** 2)  # Gaussian normalization constant\n",
        "    temp2 = (np.linalg.norm(x - mu) / sigma) ** 2       # Squared Mahalanobis distance\n",
        "    temp3 = np.exp(-0.5 * temp2)                        # Exponential factor\n",
        "    return temp * temp3\n",
        "```\n",
        "### 2.1.1 High-Level Concept: The \"Likelihood\" Calculator\n",
        "\n",
        "\\begin{align}\n",
        "G(x,\\mu,\\sigma,N) =  \\frac{1}{(\\sigma\\sqrt{2\\pi})^N} \\exp{\\frac{-{(x-\\mu)}^2}{2{\\sigma^2}}}\n",
        "\\end{align}\n",
        "\n",
        "![Source: miro.medium.com](https://miro.medium.com/v2/resize:fit:683/1*9rdkuNxjx5bCACeo1CVjpA.png)\n",
        "\n",
        "[Image Source: https://miro.medium.com/v2/resize:fit:683/1*9rdkuNxjx5bCACeo1CVjpA.png]\n",
        "\n",
        "Imagine we have a single 2D \"bell curve\" (a Gaussian distribution) floating over a graph.\n",
        "- mu: Is the (x, y) coordinate of the peak of the bell curve.\n",
        "- sigma: Is a single number that describes how \"wide\" or \"spread out\" the bell is. A smaller sigma means a sharper, narrower peak. A larger sigma means a flatter, wider peak.\n",
        "- x: Is a specific point on the graph, like [1, 2].\n",
        "\n",
        "The function `g(x, mu, sigma)` answers the question: \"What is the height of the bell curve at point x?\"\n",
        "\n",
        "This \"height\" isn't a probability (which is always between 0 and 1), but a probability density. A higher value means the point x is more \"likely\" or \"plausible\" to have come from this specific bell curve.\n",
        "\n",
        "### 2.1.2 Gaussian normalization constant\n",
        "```\n",
        "    temp = 1 / ((((2 * math.pi) ** 0.5) * sigma) ** 2)  # Gaussian normalization constant\n",
        "```\n",
        "- **Purpose:** This calculates the Normalization Term. Its job is to control the height of the bell curve's peak to ensure that the total volume under the 2D surface equals 1.\n",
        "- **Math:** The code calculates 1 / (σ√(2π)), which simplifies to `1 / √(2π * σ^2)`.\n",
        "- **Why this formula?** This is the correct normalization constant for a 2-dimensional isotropic Gaussian. For a 1D Gaussian, it would just be 1 / (σ√(2π)). The squaring (** 2) is what adapts it for 2D.\n",
        "\n",
        "### 2.1.3 Squared Mahalanobis distance\n",
        "\\begin{align}\n",
        "M = \\sqrt{\\frac{(x-\\mu)^2}{\\sigma^2}}\n",
        "\\end{align}\n",
        "\n",
        "```\n",
        "    temp2 = (np.linalg.norm(x - mu) / sigma) ** 2       # Squared Mahalanobis distance\n",
        "```\n",
        "- **Purpose**: This calculates the Squared Distance Term, which measures how far away the point x is from the center mu, scaled by the standard deviation.\n",
        "- **How it works:**\n",
        "  - `x - mu`: Calculates the vector difference between the point and the mean. e.g., `[1, 2] - [2, 3] = [-1, -1]`.\n",
        "  - `np.linalg.norm(...)`: Calculates the standard Euclidean (straight-line) distance of that vector. e.g., `norm([-1, -1])` is `sqrt((-1)^2 + (-1)^2) = sqrt(2)`.\n",
        "  - `... / sigma:` Scales this distance. It answers \"how many standard deviations away is the point?\"\n",
        "  - `(...) ** 2`: The result is squared.\n",
        "- **Formal Name:** This is a simplified version of the Squared Mahalanobis Distance for an isotropic distribution.\n",
        "\n",
        "### 2.1.4 Exponential factor\n",
        "\\begin{align}\n",
        "G(x,\\mu,\\sigma,N) =  \\frac{1}{(\\sigma\\sqrt{2\\pi})^N} \\exp{\\frac{-M^2}{2}}\n",
        "\\end{align}\n",
        "```\n",
        "    temp3 = np.exp(-0.5 * temp2)                        # Exponential factor\n",
        "```\n",
        "- **Purpose:** This creates the actual \"bell curve\" shape.\n",
        "- **How it works:**\n",
        "  - It takes the distance we just calculated (temp2) and plugs it into the exponential function e^(-0.5 * distance).\n",
        "  - If x is very close to mu, then temp2 is small,` -0.5 * temp2` is close to 0, and `exp(...)` is close to 1.\n",
        "  - If x is far from mu, then temp2 is large, `-0.5 * temp2` is a large negative number, and `exp(...)` is close to 0.\n",
        "\n",
        "### 2.1.5 Probabolity Density\n",
        "```\n",
        "    return temp * temp3\n",
        "```\n",
        "- **Purpose:** This combines the pieces.\n",
        "- **How it works:** It multiplies the peak height of the curve (temp) by the exponential decay factor (temp3). The result is the final probability density (the \"height\" of the bell curve) at point x.\n",
        "\n",
        "### 2.1.6 Usage\n",
        "- This function is a self-contained calculator for the likelihood of a single point x belonging to a single, simple Gaussian cluster defined by mu and sigma. - In the EM algorithm, we would call this function repeatedly: once for each data point and for each cluster, to see how well each point \"fits\" into each of the possible clusters.\n",
        "\n",
        "## 2.2 Calculating \"Effective\" Cluster responsibilities i.e. sizes\n",
        "```\n",
        "# Helper function: Sum every k-th value in a list\n",
        "def sum_every_kth_value_list(arr, k):\n",
        "    result = []\n",
        "    for i in range(k):\n",
        "        sum_val = sum(arr[i::k])\n",
        "        result.append(sum_val)\n",
        "    return result\n",
        "```\n",
        "This function, sum_every_kth_value_list, is a utility designed to be used within the M-Step (Maximization Step) of the EM algorithm.\n",
        "\n",
        "### 2.2.1 High-Level Goal: Calculating \"Effective\" Cluster Sizes\n",
        "In the M-Step, we need to update our model parameters (mu, sigma, pk). To do this, we first need to calculate a crucial value for each cluster, often called N_k:\n",
        "- N_k = The \"effective number of points\" assigned to cluster k.\n",
        "- This isn't a simple count. It's the sum of all the responsibilities that cluster k has across all data points.\n",
        "- For example, if the responsibilities (the gamma matrix from the E-Step) for Cluster A are [0.8, 0.1, 0.7], then N_A = 0.8 + 0.1 + 0.7 = 1.6. It's as if Cluster A is \"responsible for\" 1.6 data points.\n",
        "- This N_k value is the denominator used when updating the mean (mu) and standard deviation (sigma), and the numerator when updating the mixing weight (pk).\n",
        "\n",
        "### 2.2.2 Why This Specific Function is Needed\n",
        "We might think, \"Can't we just sum the columns of the gamma matrix from the E-Step?\" Yes, and np.sum(gamma, axis=0) would do that perfectly if gamma were a 2D matrix.\n",
        "\n",
        "This helper function exists because the code it supports is likely passing the gamma matrix as a flattened, 1D array.\n",
        "\n",
        "Let's see how. Imagine our gamma matrix from the E-Step looks like this for 3 data points and 2 clusters:\n",
        "\n",
        "```\n",
        "gamma = [[0.8, 0.2],   # Responsibilities for Point 0\n",
        "         [0.1, 0.9],   # Responsibilities for Point 1\n",
        "         [0.7, 0.3]]   # Responsibilities for Point 2\n",
        "```\n",
        "If you flatten this matrix into a 1D list (arr), it becomes:\n",
        "```\n",
        "arr = [0.8, 0.2, 0.1, 0.9, 0.7, 0.3]\n",
        "```\n",
        "Now, how do you sum the first column `(0.8 + 0.1 + 0.7)` and the second column `(0.2 + 0.9 + 0.3)` from this 1D list? That's exactly what sum_every_kth_value_list does!\n",
        "\n",
        "## 2.3 Verification method to check the integrity of the output from the E-Step.\n",
        "```\n",
        "# Helper function: Sum values in chunks of size k\n",
        "def sum_every_k_values(arr, k):\n",
        "    if k <= 0 or len(arr) % k != 0:\n",
        "        return \"Invalid input\"\n",
        "    return [sum(arr[i:i + k]) for i in range(0, len(arr), k)]\n",
        "```\n",
        "The EM algorithm can run perfectly without it. It's a diagnostic tool for the programmer.\n",
        "### 2.3.1 What the E-Step Guarantees?\n",
        "The e_step function calculates the responsibility matrix (gamma). A fundamental property of this gamma matrix is that for any given data point, its responsibilities across all clusters must sum to 1.\n",
        "\n",
        "Let's look at our gamma matrix:\n",
        "```\n",
        "gamma = [[0.8, 0.2],   # <-- Sum is 1.0. Point 0 is 80% A, 20% B.\n",
        "         [0.1, 0.9],   # <-- Sum is 1.0. Point 1 is 10% A, 90% B.\n",
        "         [0.7, 0.3]]   # <-- Sum is 1.0. Point 2 is 70% A, 30% B.\n",
        "```\n",
        "The E-Step's normalization guarantees that every row sums to 1.\n",
        "\n",
        "### 2.3.2 How This Function Checks That Guarantee?\n",
        "Now, imagine we flatten this gamma matrix into a 1D arr:\n",
        "```\n",
        "arr = [0.8, 0.2, 0.1, 0.9, 0.7, 0.3]\n",
        "```\n",
        "The function `sum_every_k_values` is designed to sum the values that correspond to each original row. It groups the flattened array into chunks and sums each chunk.\n",
        "\n",
        "## 2.4 Inspect the results for one specific data point at a time\n",
        "```\n",
        "# Helper function: Get the n-th set of k values from a list\n",
        "def get_nth_set_of_k_values(arr, k, n):\n",
        "    if k <= 0 or n <= 0:\n",
        "        return \"Invalid input\"\n",
        "    start_index = (n - 1) * k\n",
        "    end_index = start_index + k\n",
        "    return arr[start_index:end_index]\n",
        "```\n",
        "This helper function, get_nth_set_of_k_values, is a utility designed to work with the flattened output of the E-Step. Its primary purpose is to inspect or process the results for one specific data point at a time.\n",
        "\n",
        "\n",
        "### 2.4.1 High-Level Goal: \"Zooming In\" on a Single Data Point\n",
        "- Imagine the E-Step has just finished. It produced a gamma matrix of responsibilities, which we then flattened into a single long list (arr).\n",
        "- The purpose of this function is to answer the question: \"What were the calculated responsibilities for the n-th data point?\"\n",
        "- It allows you to \"un-flatten\" the array for just one data point of interest.\n",
        "\n",
        "### 2.4.2 Why It's Needed?\n",
        "- This function is necessary because the data is stored in a non-intuitive, flattened format.\n",
        "- If gamma were a 2D matrix, getting the responsibilities for the 3rd data point (index 2) would be simple: gamma[2, :].\n",
        "- But since gamma is flattened into arr, we can't use simple indexing. We need a function to calculate the correct starting and ending position of the 3rd data point's information within that long list.\n",
        "\n",
        "## 2.5 E Step\n",
        "```\n",
        "# E-step: Calculate numerators for updating probabilities\n",
        "numerators = []\n",
        "for j in range(0, mu.shape[0]):  # Iterate over each Gaussian component\n",
        "    for i in range(0, x.shape[0]):  # Iterate over each data point\n",
        "        value = g(x[i], mu[j], sigma[0][j]) * pk[0][j]\n",
        "        numerators.append(value)\n",
        "\n",
        "# Compute denominators for normalizing probabilities\n",
        "denominators = sum_every_kth_value_list(numerators, int(len(numerators) / 2))\n",
        "\n",
        "# Update probabilities for each data point and each component\n",
        "new_p = []\n",
        "for i in range(0, len(numerators)):\n",
        "    new_p.append(numerators[i] / denominators[i % int(len(numerators) / 2)])\n",
        "\n",
        "# Compute p_k_n (updated prior probabilities)\n",
        "p_k_n = sum_every_k_values(new_p, int(len(numerators) / 2))\n",
        "```\n",
        "\n",
        "This block of code is a fascinating and highly procedural implementation of the entire E-Step and the very first calculation of the M-Step.\n",
        "\n",
        "### 2.5.1 Conceptual Overview\n",
        "Let's map the code variables to their mathematical meaning in the EM algorithm. This is the key to understanding the code.\n",
        "\n",
        "\n",
        "| Code Variable  | Mathematical Meaning                                       | Role in E-M Algorithm          |\n",
        "| :------------- | :--------------------------------------------------------- | :----------------------------- |\n",
        "| `numerators`   | The joint probability `P(x_i, C_j)` for every point `i` and cluster `j`. This is `P(x_i \\| C_j) * P(C_j)`. | Intermediate step in E-Step.   |\n",
        "| `denominators` | The marginal probability `P(x_i)` for every point `i`. This is `Sum over all k of P(x_i, C_k)`. | Intermediate step in E-Step.   |\n",
        "| `new_p`        | The responsibility `P(C_j \\| x_i)`. This is `P(x_i, C_j) / P(x_i)`. | **Final output of the E-Step.** This is the `gamma` matrix, but flattened. |\n",
        "| `p_k_n`        | The \"effective number of points\" `N_k` for each cluster `k`. This is `Sum over all i of P(C_k \\| x_i)`. | **First calculation of the M-Step.** |\n",
        "\n",
        "\n",
        "### 2.5.2 Calculating the Numerators (Joint Probabilities)\n",
        "```\n",
        "# E-step: Calculate numerators for updating probabilities\n",
        "numerators = []\n",
        "for j in range(0, mu.shape[0]):  # Iterate over each Gaussian component (cluster)\n",
        "    for i in range(0, x.shape[0]):  # Iterate over each data point\n",
        "        value = g(x[i], mu[j], sigma[0][j]) * pk[0][j]\n",
        "        numerators.append(value)\n",
        "```\n",
        "- **Goal**: Calculate P(x_i, C_j), the joint probability of seeing data point i AND it belonging to cluster j.\n",
        "- **How it Works**: The code loops through each cluster first, and then through each data point. For each (point, cluster) pair, it calculates g(...) (the likelihood P(x_i|C_j)) and multiplies it by the cluster's prior probability pk (P(C_j)).\n",
        "- **The Critical Detail (Data Layout):** Because the outer loop is over clusters, the numerators list is ordered in a \"cluster-major\" format. For 2 clusters (C0, C1) and 3 data points (x0, x1, x2), the list will look like:\n",
        "[P(x0,C0), P(x1,C0), P(x2,C0), P(x0,C1), P(x1,C1), P(x2,C1)]\n",
        "\n",
        "### 2.5.3 Calculating the Denominators (Marginal Probabilities)\n",
        "```\n",
        "# Compute denominators for normalizing probabilities\n",
        "denominators = sum_every_kth_value_list(numerators, int(len(numerators) / 2))\n",
        "```\n",
        "- **Goal:** Calculate P(x_i), the total probability of observing data point i, regardless of which cluster it came from. This is the normalization factor needed for the E-Step. Mathematically, P(x_i) = P(x_i, C0) + P(x_i, C1).\n",
        "- **How it Works:** This is where the cleverness comes in. It uses sum_every_kth_value_list on the strangely ordered numerators list. With k = N (number of data points), it correctly groups the values for each point.\n",
        "  - The first element of denominators will be numerators[0] + numerators[N], which is P(x0,C0) + P(x0,C1) = P(x0).\n",
        "  - The second element will be numerators[1] + numerators[N+1], which is P(x1,C0) + P(x1,C1) = P(x1). And so on.\n",
        "- **Result:** denominators is a list [P(x0), P(x1), P(x2), ...].\n",
        "\n",
        "### 2.5.4 Calculating new_p (The Responsibilities) - The E-Step Conclusion\n",
        "```\n",
        "# Update probabilities for each data point and each component\n",
        "new_p = []\n",
        "for i in range(0, len(numerators)):\n",
        "    new_p.append(numerators[i] / denominators[i % int(len(numerators) / 2)])\n",
        "```\n",
        "- **Goal**: This is the final step of the E-Step. It calculates the responsibility P(C_j | x_i), which is the probability of cluster j given data point i.\n",
        "- **How it Works**: It divides each element in numerators by the corresponding element in denominators. The i % N logic correctly maps the i-th element of the \"cluster-major\" numerators list to the correct denominator for its data point.\n",
        "  - e.g., numerators[N] (which is P(x0,C1)) is divided by denominators[0] (which is P(x0)) to get P(C1|x0).\n",
        "- **Result:** new_p is the final, flattened responsibility matrix (gamma). This concludes the E-Step.\n",
        "\n",
        "### 2.5.5 Calculating p_k_n (Effective Cluster Sizes) - The M-Step Begins\n",
        "```\n",
        "# Compute p_k_n (updated prior probabilities)\n",
        "p_k_n = sum_every_k_values(new_p, int(len(numerators) / 2))\n",
        "```\n",
        "- **Goal:** This is the first calculation of the M-Step. It computes N_k, the \"effective number of points\" in each cluster. This is done by summing up the responsibilities for each cluster over all data points.\n",
        "- **How it Works:** It uses sum_every_k_values to sum the new_p list in chunks.\n",
        "  - The first chunk is the first N elements of new_p, which are all the responsibilities for Cluster 0: [P(C0|x0), P(C0|x1), P(C0|x2), ...]. Summing them gives N_0.\n",
        "  - The second chunk gives N_1, and so on.\n",
        "- **Result:** p_k_n (a slightly confusing name) is a list [N_0, N_1, ...]. This value is the crucial denominator for updating the means and standard deviations in the rest of the M-Step. It is also used to update the mixing weights pk.\n",
        "\n",
        "\n",
        "## 2.6 M-Step (Maximization Step)\n",
        "\n",
        "```\n",
        "# M-step: Update the means (mu) and standard deviations (sigma)\n",
        "new_mu = []\n",
        "new_std = []\n",
        "for i in range(1, 3):  # Iterate over components\n",
        "    temp = 0\n",
        "    temp2 = 0\n",
        "    for j in range(0, x.shape[0]):  # Update mean\n",
        "        temp += get_nth_set_of_k_values(new_p, 4, i)[j] * x[j]\n",
        "    temp = temp / p_k_n[i - 1]\n",
        "    new_mu.append(temp)\n",
        "\n",
        "    for k in range(0, x.shape[0]):  # Update standard deviation\n",
        "        temp2 += get_nth_set_of_k_values(new_p, 4, i)[k] * (np.linalg.norm(x[k] - new_mu[i - 1]) ** 2)\n",
        "    temp2 = (temp2 / (2 * p_k_n[i - 1])) ** 0.5\n",
        "    new_std.append(temp2)\n",
        "\n",
        "# Update prior probabilities\n",
        "updated_p = []\n",
        "for i in range(0, 2):\n",
        "    updated_p.append(p_k_n[i] / int(len(numerators) / 2))\n",
        "\n",
        "# Final updated parameters: new means, new standard deviations, and updated priors\n",
        "new_mu, new_std, updated_p\n",
        "```\n",
        "This block of code is the implementation of the M-Step (Maximization Step), the second major phase of the E-M algorithm cycle.\n",
        "\n",
        "### 2.6.1 High-Level Goal: The \"Updating\" Step\n",
        "- The purpose of the M-Step is to take the \"soft assignments\" or responsibilities calculated in the E-Step (new_p) and use them to compute a new, better set of model parameters. It \"maximizes\" the likelihood of the data given these responsibilities.\n",
        "1. This specific code updates two of the three parameters:\n",
        "  - mu (the means of the clusters)\n",
        "  - sigma (the standard deviations of the clusters)\n",
        "2. It does this by calculating a weighted average for each parameter.\n",
        "\n",
        "### 2.6.2 Conceptual Formulas\n",
        "The code is implementing the following update rules:\n",
        "1. New Mean (μ_k): The new center of a cluster k is the weighted average of all data points, where the weight for each point is its responsibility to that cluster.\n",
        "  - new_μ_k = ( Σ [ P(C_k|x_i) * x_i ] ) / ( Σ [ P(C_k|x_i) ] )\n",
        "  - The denominator is simply N_k, which was already calculated as p_k_n.\n",
        "2. New Variance (σ_k²): The new variance of a cluster k is the weighted average of the squared distances of all points from the cluster's new mean.\n",
        "  - new_σ_k² = ( Σ [ P(C_k|x_i) * ||x_i - new_μ_k||² ] ) / ( D * Σ [ P(C_k|x_i) ] )\n",
        "Where D is the number of dimensions of the data (in this case, D=2). The code then takes the square root to get the standard deviation σ_k.\n",
        "\n",
        "### 2.6.3 Updating the Mixing Weights (pk)\n",
        "```\n",
        "updated_p = []\n",
        "for i in range(0, 2):\n",
        "    updated_p.append(p_k_n[i] / int(len(numerators) / 2))\n",
        "```\n",
        "\n",
        "- The mixing weight (p_k or π_k) represents our belief about the proportion of the entire dataset that belongs to a specific cluster k.\n",
        "- For example, if we have two clusters, A and B, their mixing weights might be 0.4 and 0.6, meaning we believe 40% of the data comes from cluster A and 60% from cluster B.\n",
        "- The goal of this code is to update these proportions based on the results of the E-Step.\n",
        "\n",
        "### 2.6.4 Conceptual Formula\n",
        "The update rule for the new mixing weight of a cluster k is very intuitive:\n",
        "\n",
        "```\n",
        "new_p_k = (Effective number of points in cluster k) / (Total number of points)\n",
        "```\n",
        "Mathematically, this is written as:\n",
        "```\n",
        "new_p_k = N_k / N\n",
        "```\n",
        "where:\n",
        "- N_k = Σ [ P(C_k|x_i) ] (The sum of responsibilities for cluster k over all data points i).\n",
        "- N = Total number of data points in the dataset.\n",
        "\n",
        "### 2.6.5 final results of one full E-M iteration.\n",
        "- new_mu: The list of updated means calculated in the previous code block.\n",
        "- new_std: The list of updated standard deviations calculated in the previous code block.\n",
        "- updated_p: The list of updated mixing weights calculated just now.\n",
        "\n"
      ],
      "metadata": {
        "id": "CBcBBFzn8vuz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Extend the Algorithm\n",
        "Modify the code to run over multiple iterations. Implement:\n",
        "- **A maximum iteration count** to limit the number of iterations.\n",
        "- **A stopping criterion** based on convergence (e.g., a small change in parameter values between iterations)."
      ],
      "metadata": {
        "id": "CMbGSQoGhnEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Iterative E-M Algorithm ---\n",
        "# Define iteration controls\n",
        "max_iterations = 100\n",
        "convergence_threshold = 1e-5 # Stop when the change in means is very small\n",
        "\n",
        "print(\"--- Initial Guesses for parametrs---\")\n",
        "print(f\"Initial Means (mu):\\n{mu}\")\n",
        "print(f\"Initial Std Devs (sigma):\\n{sigma}\")\n",
        "print(f\"Initial Priors (pk):\\n{pk}\")\n",
        "\n",
        "# Main loop for the E-M algorithm\n",
        "for iteration in range(max_iterations):\n",
        "    # Store old means to check for convergence later\n",
        "    old_mu = mu.copy()\n",
        "\n",
        "    # --- E-Step ---\n",
        "    # (The logic here is identical to the original code)\n",
        "    numerators = []\n",
        "    # Note: mu.shape[0] correctly gets the number of clusters (k)\n",
        "    for j in range(0, mu.shape[0]):\n",
        "        for i in range(0, x.shape[0]):\n",
        "            value = g(x[i], mu[j], sigma[0][j]) * pk[0][j]\n",
        "            numerators.append(value)\n",
        "\n",
        "    # Denominators are calculated based on the number of data points\n",
        "    num_points = x.shape[0]\n",
        "    denominators = sum_every_kth_value_list(numerators, num_points)\n",
        "\n",
        "    # Update probabilities for each data point and each component\n",
        "    new_p = []\n",
        "    for i in range(0, len(numerators)):\n",
        "        new_p.append(numerators[i] / denominators[i % num_points])\n",
        "\n",
        "    # Compute p_k_n (updated prior probabilities)\n",
        "    p_k_n = sum_every_k_values(new_p, num_points)\n",
        "\n",
        "    # --- M-Step ---\n",
        "    # (The logic here is identical to the original code, but we use variables for clarity)\n",
        "    new_mu = []\n",
        "    new_std = []\n",
        "    num_clusters = mu.shape[0]\n",
        "\n",
        "    # Note: This loop was hardcoded for 2 clusters. A more general solution\n",
        "    # would be `for i in range(num_clusters):`\n",
        "    for i in range(1, num_clusters + 1):\n",
        "        temp = 0\n",
        "        temp2 = 0\n",
        "        for j in range(0, x.shape[0]):\n",
        "            # Note: The '4' here was hardcoded for 4 data points.\n",
        "            temp += get_nth_set_of_k_values(new_p, num_points, i)[j] * x[j]\n",
        "        temp = temp / p_k_n[i - 1]\n",
        "        new_mu.append(temp)\n",
        "\n",
        "        for k in range(0, x.shape[0]):\n",
        "            # Note: The '4' here was hardcoded for 4 data points.\n",
        "            temp2 += get_nth_set_of_k_values(new_p, num_points, i)[k] * (np.linalg.norm(x[k] - new_mu[i - 1]) ** 2)\n",
        "        # Note: The '2' here is hardcoded for 2 dimensions.\n",
        "        temp2 = (temp2 / (2 * p_k_n[i - 1])) ** 0.5\n",
        "        new_std.append(temp2)\n",
        "\n",
        "    # Update prior probabilities\n",
        "    updated_p = []\n",
        "    for i in range(0, num_clusters):\n",
        "        updated_p.append(p_k_n[i] / num_points)\n",
        "\n",
        "    # --- Convergence Check & Parameter Update ---\n",
        "\n",
        "    # Calculate the change in means. np.linalg.norm computes the\n",
        "    #      Euclidean distance between the flattened old and new mean matrices.\n",
        "    change = np.linalg.norm(np.array(new_mu) - old_mu)\n",
        "\n",
        "    print(f\"Iteration {iteration + 1}: Change in means = {change:.6f}\")\n",
        "\n",
        "    # Update the parameters for the next iteration\n",
        "    # We must convert the lists back to numpy arrays with the correct shapes\n",
        "    mu = np.array(new_mu)\n",
        "    sigma = np.array([new_std])\n",
        "    pk = np.array([updated_p])\n",
        "\n",
        "    # NEW: Check if the change is below the threshold\n",
        "    if change < convergence_threshold:\n",
        "        print(f\"\\nConvergence reached after {iteration + 1} iterations.\")\n",
        "        break\n",
        "    if iteration == max_iterations: # This belongs to the 'for' loop, runs if the loop finishes without break\n",
        "        print(f\"\\nMaximum number of iterations ({max_iterations}) reached without convergence.\")\n",
        "\n",
        "# --- Final Results ---\n",
        "print(\"\\n--- Final Learned Parameters ---\")\n",
        "print(f\"Final Means (mu):\\n{mu}\")\n",
        "print(f\"Final Std Devs (sigma):\\n{sigma}\")\n",
        "print(f\"Final Priors (pk):\\n{pk}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONR0Zh6MkM4R",
        "outputId": "0fd76f04-cdb2-4815-c0db-fa2c3c7f65b8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initial Guesses for parametrs---\n",
            "Initial Means (mu):\n",
            "[[2.17662611 2.3922087 ]\n",
            " [3.75694927 2.91898309]]\n",
            "Initial Std Devs (sigma):\n",
            "[[1.15470054 1.15470054]]\n",
            "Initial Priors (pk):\n",
            "[[0.5 0.5]]\n",
            "Iteration 1: Change in means = 0.684225\n",
            "Iteration 2: Change in means = 0.596798\n",
            "Iteration 3: Change in means = 0.107111\n",
            "Iteration 4: Change in means = 0.000001\n",
            "\n",
            "Convergence reached after 4 iterations.\n",
            "\n",
            "--- Final Learned Parameters ---\n",
            "Final Means (mu):\n",
            "[[1.  2.5]\n",
            " [4.  2.5]]\n",
            "Final Std Devs (sigma):\n",
            "[[0.35355339 0.35355339]]\n",
            "Final Priors (pk):\n",
            "[[0.5 0.5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observations\n",
        "The EM algorithm performed exceptionally well on this dataset. It demonstrated perfect convergence solution in just 4 iterations.\n",
        "\n",
        "# Assigning each data point to a cluster\n",
        "Let's conclude the clustering conceptually first.\n",
        "\n",
        "1. **Take the Final Parameters:** Use the converged mu, sigma, and pk values. These represent the \"perfect\" model of our data's structure.\n",
        "2. **Calculate Final Likelihoods:** For each data point, calculate its   likelihood of belonging to each of the two learned clusters.\n",
        "  - Likelihood_A = P(point | Cluster_A) calculated using g(point, mu_A, sigma_A).\n",
        "  - Likelihood_B = P(point | Cluster_B) calculated using g(point, mu_B, sigma_B).\n",
        "3. **Calculate Final Probabilities (Responsibilities):** Convert these likelihoods into true probabilities by also considering the mixing weights (pk).\n",
        "  - Responsibility_A = pk_A * Likelihood_A\n",
        "  - Responsibility_B = pk_B * Likelihood_B\n",
        "  - Then, normalize these so they sum to 1 for each point.\n",
        "4. **Make the Hard Assignment:** For each data point, assign it to the cluster for which it has the highest final probability (responsibility).\n",
        "\n",
        "# Clustering Implementation (Code)\n",
        "\n",
        "The algorithm perfectly partitioned the data into two distinct, non-overlapping groups based on the values in the first feature.\""
      ],
      "metadata": {
        "id": "LBDcGsYotJww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Concluding the Clustering ---\n",
        "print(\"\\n--- Final Cluster Assignments ---\")\n",
        "\n",
        "# The final learned parameters from the loop\n",
        "final_mu = mu\n",
        "final_sigma = sigma\n",
        "final_pk = pk\n",
        "\n",
        "# Create an empty list to store the cluster assignments for each data point\n",
        "assignments = []\n",
        "\n",
        "# Iterate through each data point to assign it to a cluster\n",
        "for point in x:\n",
        "    # Calculate the unnormalized probability (likelihood * prior) for each cluster\n",
        "    prob_in_cluster_0 = final_pk[0, 0] * g(point, final_mu[0], final_sigma[0, 0])\n",
        "    prob_in_cluster_1 = final_pk[0, 1] * g(point, final_mu[1], final_sigma[0, 1])\n",
        "\n",
        "    # Assign the point to the cluster with the higher probability\n",
        "    if prob_in_cluster_0 > prob_in_cluster_1:\n",
        "        assignments.append(0) # Assign to Cluster 0\n",
        "    else:\n",
        "        assignments.append(1) # Assign to Cluster 1\n",
        "\n",
        "# Print the results in a clear format\n",
        "print(\"Data Point\\t ->\\tAssigned Cluster\")\n",
        "print(\"---------------------------------------\")\n",
        "for i, point in enumerate(x):\n",
        "    print(f\"{point}\\t ->\\tCluster {assignments[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fO1_D0-Avu32",
        "outputId": "bb18f356-e68c-4826-b763-b2da16a1cf4f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Cluster Assignments ---\n",
            "Data Point\t ->\tAssigned Cluster\n",
            "---------------------------------------\n",
            "[1 2]\t ->\tCluster 0\n",
            "[4 2]\t ->\tCluster 1\n",
            "[1 3]\t ->\tCluster 0\n",
            "[4 3]\t ->\tCluster 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Zme85YG4wH3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Part 2: Analyzing Runtime\n",
        "1. Import Python's `time` module and measure the runtime of each section of the code:\n",
        "   - **Initialization**: Measure the time to compute the initial `mu`, `sigma`, and `pk`.\n",
        "   - **E-Step**: Measure the time to compute probabilities and numerators.\n",
        "   - **M-Step**: Measure the time to update the parameters.\n",
        "\n",
        "2. Add comments next to each runtime measurement to document how long the operation took.\n",
        "\n",
        "3. Using the runtimes, analyze the algorithm's complexity:\n",
        "   - Consider \\(n\\), the number of data points.\n",
        "   - Consider \\(k\\), the number of Gaussian components.\n",
        "   - Derive the overall runtime complexity as a function of \\(n\\) and \\(k\\)."
      ],
      "metadata": {
        "id": "9RB6rR25wP1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time # Import the time module\n",
        "\n",
        "init_start_time = time.time() # Start timer for initialization\n",
        "\n",
        "# Initialize means (mu) based on the data\n",
        "mu = ((column_means.reshape(1, -1) * np.array([1, 1]).reshape(-1, 1)) +\n",
        "      (column_stddevs.reshape(1, -1) * np.array([-0.1867, 0.7257]).reshape(-1, 1)))\n",
        "\n",
        "# Initialize standard deviations (sigma) for cluster\n",
        "sigma = std_deviations.reshape(1, -1) * np.array([1, 1]).reshape(1, -1)\n",
        "\n",
        "# Initialize the prior probabilities (pk) i.e. Mixing Weights\n",
        "pk = np.array([1, 1]).reshape(1, -1) / 2\n",
        "\n",
        "init_end_time = time.time() # End timer for initialization\n",
        "print(f\"--- Parameter initialization took: {init_end_time - init_start_time:.6f} seconds ---\")\n",
        "\n",
        "\n",
        "# Define iteration controls\n",
        "max_iterations = 100\n",
        "convergence_threshold = 1e-5 # Stop when the change in means is very small\n",
        "\n",
        "print(\"--- Initial Guesses for parametrs---\")\n",
        "print(f\"Initial Means (mu):\\n{mu}\")\n",
        "print(f\"Initial Std Devs (sigma):\\n{sigma}\")\n",
        "print(f\"Initial Priors (pk):\\n{pk}\")\n",
        "\n",
        "# Main loop for the E-M algorithm\n",
        "total_e_step_time = 0\n",
        "total_m_step_time = 0\n",
        "\n",
        "for iteration in range(max_iterations):\n",
        "    # Store old means to check for convergence later\n",
        "    old_mu = mu.copy()\n",
        "\n",
        "    # --- E-Step ---\n",
        "    e_step_start_time = time.time() # Start E-Step timer\n",
        "\n",
        "    # (The logic here is identical to the original code)\n",
        "    numerators = []\n",
        "    # Note: mu.shape[0] correctly gets the number of clusters (k)\n",
        "    for j in range(0, mu.shape[0]): # Runtime complexity: Run k times\n",
        "        for i in range(0, x.shape[0]): # Runtime complexity: Run n times\n",
        "            value = g(x[i], mu[j], sigma[0][j]) * pk[0][j] # Run time complexity: Run d times (size of feature)\n",
        "            numerators.append(value)\n",
        "    # Runtime complexity for E step: O(k*n*d)\n",
        "    # Denominators are calculated based on the number of data points\n",
        "    num_points = x.shape[0]\n",
        "    denominators = sum_every_kth_value_list(numerators, num_points)\n",
        "\n",
        "    # Update probabilities for each data point and each component\n",
        "    new_p = []\n",
        "    for i in range(0, len(numerators)):\n",
        "        new_p.append(numerators[i] / denominators[i % num_points])\n",
        "\n",
        "    # Compute p_k_n (updated prior probabilities)\n",
        "    p_k_n = sum_every_k_values(new_p, num_points)\n",
        "\n",
        "    e_step_end_time = time.time() # NEW: End E-Step timer\n",
        "    total_e_step_time += (e_step_end_time - e_step_start_time)\n",
        "\n",
        "\n",
        "    # --- M-Step ---\n",
        "    m_step_start_time = time.time() # Start M-Step timer\n",
        "    # (The logic here is identical to the original code, but we use variables for clarity)\n",
        "    new_mu = []\n",
        "    new_std = []\n",
        "    num_clusters = mu.shape[0]\n",
        "\n",
        "    # Note: This loop was hardcoded for 2 clusters. A more general solution\n",
        "    # would be `for i in range(num_clusters):`\n",
        "    for i in range(1, num_clusters + 1): # Run time complexity: Run k times\n",
        "        temp = 0\n",
        "        temp2 = 0\n",
        "        for j in range(0, x.shape[0]): # Run time complexity: Run n times\n",
        "            # Note: The '4' here was hardcoded for 4 data points.\n",
        "            temp += get_nth_set_of_k_values(new_p, num_points, i)[j] * x[j] # run d times\n",
        "        temp = temp / p_k_n[i - 1]\n",
        "        new_mu.append(temp)\n",
        "\n",
        "        for k in range(0, x.shape[0]): # Run n times\n",
        "            # Note: The '4' here was hardcoded for 4 data points.\n",
        "            temp2 += get_nth_set_of_k_values(new_p, num_points, i)[k] * (np.linalg.norm(x[k] - new_mu[i - 1]) ** 2) # Run d times\n",
        "        # Note: The '2' here is hardcoded for 2 dimensions.\n",
        "        temp2 = (temp2 / (2 * p_k_n[i - 1])) ** 0.5\n",
        "        new_std.append(temp2)\n",
        "\n",
        "    # Update prior probabilities\n",
        "    updated_p = []\n",
        "    for i in range(0, num_clusters):\n",
        "        updated_p.append(p_k_n[i] / num_points)\n",
        "\n",
        "    m_step_end_time = time.time() # End M-Step timer\n",
        "    total_m_step_time += (m_step_end_time - m_step_start_time)\n",
        "\n",
        "    # --- Convergence Check & Parameter Update ---\n",
        "\n",
        "    # Calculate the change in means. np.linalg.norm computes the\n",
        "    #      Euclidean distance between the flattened old and new mean matrices.\n",
        "    change = np.linalg.norm(np.array(new_mu) - old_mu)\n",
        "\n",
        "    print(f\"Iteration {iteration + 1}: Change in means = {change:.6f}\")\n",
        "\n",
        "    # Update the parameters for the next iteration\n",
        "    # We must convert the lists back to numpy arrays with the correct shapes\n",
        "    mu = np.array(new_mu)\n",
        "    sigma = np.array([new_std])\n",
        "    pk = np.array([updated_p])\n",
        "\n",
        "    # NEW: Check if the change is below the threshold\n",
        "    if change < convergence_threshold:\n",
        "        print(f\"\\nConvergence reached after {iteration + 1} iterations.\")\n",
        "        break\n",
        "    if iteration == max_iterations: # This belongs to the 'for' loop, runs if the loop finishes without break\n",
        "        print(f\"\\nMaximum number of iterations ({max_iterations}) reached without convergence.\")\n",
        "\n",
        "# --- Final Results ---\n",
        "print(\"\\n--- Final Learned Parameters ---\")\n",
        "print(f\"Final Means (mu):\\n{mu}\")\n",
        "print(f\"Final Std Devs (sigma):\\n{sigma}\")\n",
        "print(f\"Final Priors (pk):\\n{pk}\")\n",
        "\n",
        "# NEW: Print runtime summary\n",
        "print(\"\\n--- Runtime Analysis ---\")\n",
        "print(f\"Total E-Step time over {iteration+1} iterations: {total_e_step_time:.6f} seconds\")\n",
        "print(f\"Total M-Step time over {iteration+1} iterations: {total_m_step_time:.6f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HF1Xe2xYxDsX",
        "outputId": "6fb9576b-9800-4b8c-8499-9a6be87980d8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Parameter initialization took: 0.000373 seconds ---\n",
            "--- Initial Guesses for parametrs---\n",
            "Initial Means (mu):\n",
            "[[2.17662611 2.3922087 ]\n",
            " [3.75694927 2.91898309]]\n",
            "Initial Std Devs (sigma):\n",
            "[[1.15470054 1.15470054]]\n",
            "Initial Priors (pk):\n",
            "[[0.5 0.5]]\n",
            "Iteration 1: Change in means = 0.684225\n",
            "Iteration 2: Change in means = 0.596798\n",
            "Iteration 3: Change in means = 0.107111\n",
            "Iteration 4: Change in means = 0.000001\n",
            "\n",
            "Convergence reached after 4 iterations.\n",
            "\n",
            "--- Final Learned Parameters ---\n",
            "Final Means (mu):\n",
            "[[1.  2.5]\n",
            " [4.  2.5]]\n",
            "Final Std Devs (sigma):\n",
            "[[0.35355339 0.35355339]]\n",
            "Final Priors (pk):\n",
            "[[0.5 0.5]]\n",
            "\n",
            "--- Runtime Analysis ---\n",
            "Total E-Step time over 4 iterations: 0.000780 seconds\n",
            "Total M-Step time over 4 iterations: 0.000742 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Runtime Analysis\n",
        "When running the code on given dataset (4 points), it measured total time as below for E & M Steps.\n",
        "\n",
        "```\n",
        "Total E-Step time over 4 iterations: 0.000787 seconds\n",
        "Total M-Step time over 4 iterations: 0.000615 seconds\n",
        "```\n",
        "## Algorithm Complexity\n",
        "Let's define following variables\n",
        "- n: The number of data points (x.shape[0]).\n",
        "- k: The number of Gaussian components, or clusters (mu.shape[0]).\n",
        "- d: The number of dimensions, or features, of each data point (x.shape[1]).\n",
        "- I : The number of iterations until convergence.\n",
        "\n",
        "### E-Step Complexity (per iteration):\n",
        "\n",
        "We have three loops\n",
        "- For for components k\n",
        "- Second for data points n\n",
        "- Third for size of features for PDF function `g` d\n",
        "\n",
        "Therefore overall time complexity would be O(k.n.d)\n",
        "\n",
        "### M-Step Complexity (per iteration):\n",
        "We have three loops\n",
        "- For for components k\n",
        "- Second for data points n\n",
        "- Third for size of features for function `get_nth_set_of_k_values` d\n",
        "\n",
        "Therefore overall time complexity would be O(k.n.d)\n",
        "\n",
        "### Overall Runtime Complexity\n",
        "Since iterative E-M code can run maximum `I` times therefore overall time complexity will be O(I.k.n.d)\n",
        "\n"
      ],
      "metadata": {
        "id": "alaBiu0WzgYY"
      }
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.9.1 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}